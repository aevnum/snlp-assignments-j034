{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4484bb",
   "metadata": {},
   "source": [
    "# IMDB Movie Reviews Sentiment Classification\n",
    "\n",
    "This notebook finetunes and compares multiple transformer models on the IMDB movie reviews dataset. The best model is then further finetuned on the full dataset, and inference is run on random test samples.\n",
    "\n",
    "## Plan\n",
    "\n",
    "1. **Setup & Imports**\n",
    "   - Install and import required libraries (transformers, datasets, sklearn, torch, etc.)\n",
    "\n",
    "2. **Data Loading & Preprocessing**\n",
    "   - Download and load the IMDB dataset\n",
    "   - Preprocess the data (tokenization, train/val/test split, label encoding)\n",
    "\n",
    "3. **Custom F1 Score Function**\n",
    "   - Implement a custom F1 score function for evaluation\n",
    "\n",
    "4. **Model Selection**\n",
    "   - Select 5 different encoder models (from both old and new models)\n",
    "\n",
    "5. **Finetuning on Subset**\n",
    "   - For each model:\n",
    "     - Finetune on a subset of the training data\n",
    "     - Evaluate on the validation set using the custom F1 score\n",
    "\n",
    "6. **Model Comparison**\n",
    "   - Compare the F1 scores of all models\n",
    "   - Select the best performing model\n",
    "\n",
    "7. **Finetuning Best Model on Full Dataset**\n",
    "   - Finetune the best model on the entire training set\n",
    "   - Evaluate on the test set\n",
    "\n",
    "8. **Inference on Random Samples**\n",
    "   - Randomly sample 10 reviews from the test set\n",
    "   - Run inference using the best model\n",
    "   - Display the reviews, predicted labels, and true labels\n",
    "\n",
    "9. **Conclusion**\n",
    "   - Summarize findings and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbb86a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments)\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fff113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB dataset from Kaggle using kagglehub\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews')\n",
    "\n",
    "# The CSV file is inside the downloaded directory\n",
    "csv_path = os.path.join(path, 'IMDB Dataset.csv')\n",
    "imdb = pd.read_csv(csv_path)\n",
    "print(imdb.head())\n",
    "print(imdb['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47540df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40500\n",
      "Validation size: 4500\n",
      "Test size: 5000\n",
      "Sample review: Lately I have been watching a lot of Tom Hanks films and old Chaplin films and even some of Rowan Atkinson's early Bean performances, and it seems that all of them have their own unique charm that permeates throughout their work, something that allows them to identify with audience members of all ages, in a way that just makes you feel good. A Bug's Life has that same charm, it has a connection with real life that allows us to easily suspend disbelief and accept a lot of talking insects, because even though they talk, they still ACT just like real bugs. It's like the team that made the movie found a way to bring us into the mind of a child and allow us to think like them, to imagine bugs the way a young mind does.<br /><br />Honey, I Shrunk The Kids was one of my favorite films when I was younger, and to me, A Bug's Life is like a more realistic version of that movie, if only because the animation is so breathtaking and this style of story-telling just opens up so many more narrative possibilities. I try not to compare it to something like Toy Story (which I still maintain is the best computer animated film ever made), because the story of A Bug's Life is not quite as good as Toy Story's, but then again, almost nothing is. The important thing is that it is still wonderful fun. <br /><br />The story concerns a colony of hard working bugs who have an impressively developed society, mostly geared around building a harvest of food, most of which will go to the tyrannical grasshoppers, vastly superior in strength and general meanness, and hopefully still leave enough left over for the bugs to make it through the winter. We are treated to some visits from the grasshoppers, who make it clear that if the bugs provide an unsatisfactory quantity of food, the consequences will be dire. And incidentally, the similarities between this crippling level of food extraction is strikingly similar to Mao Tse-tung's vicious forcing of food from his own people during the \"Great Leap Forward\"<br /><br />The fun and excitement begins when Flik, the main character, sets off on a quest to find a gang of appropriate warrior bugs to come back and help defend the colony against the grasshoppers. You see, he spilled all of the amassed food and placed everyone in great danger, so he feels it's his responsibility, but he inadvertently ends up hiring a struggling group of insect circus performers. Great for the audience, not so great for the safety of the clan. <br /><br />The movie was released back in the late 90s, when so many films seemed to have been coming out in twos, like Armageddon and Deep Impact, Independence Day and The Arrival, A Bug's Life and Antz, etc. Comparisons between A Bug's Life and Antz are inevitable, although it seems clear to me that A Bug's Life is by far the superior film, and not only because it doesn't star Woody Allen stuttering and whining through the lead role. This is great family fun!\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Split the Kaggle IMDB dataset using sklearn's train_test_split and convert to Hugging Face Datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Encode sentiment labels to integers\n",
    "imdb['label'] = imdb['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split into train and test (90% train, 10% test)\n",
    "train_df, test_df = train_test_split(imdb, test_size=0.1, random_state=42, stratify=imdb['label'])\n",
    "\n",
    "# Further split train into train/val (90% train, 10% val of the original train)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print('Train size:', len(train_dataset))\n",
    "print('Validation size:', len(val_dataset))\n",
    "print('Test size:', len(test_dataset))\n",
    "\n",
    "# Example sample\n",
    "print('Sample review:', train_dataset[0]['review'])\n",
    "print('Label:', train_dataset[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f53e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom F1 score function for HuggingFace Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7ef87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names (old and new)\n",
    "model_names = {\n",
    "    'bert': 'google-bert/bert-base-uncased',\n",
    "    'roberta': 'FacebookAI/roberta-base',\n",
    "    'deberta': 'microsoft/deberta-v3-base',  \n",
    "    'electra': 'google/electra-small-discriminator',\n",
    "    'distilbert': 'distilbert/distilbert-base-uncased',\n",
    "    # Newer models\n",
    "    'modernbert': 'answerdotai/ModernBERT-base',\n",
    "    'ettin': 'jhu-clsp/ettin-encoder-17m',\n",
    "    'gte': 'thenlper/gte-small'\n",
    "}\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    return tokenizer(examples['review'], truncation=True, padding='max_length', max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc816f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Finetuning google-bert/bert-base-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshit\\.conda\\envs\\snlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kshit\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1803.48 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1968.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/250 01:39 < 01:07, 1.49 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.652537</td>\n",
       "      <td>0.479534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.495200</td>\n",
       "      <td>0.331605</td>\n",
       "      <td>0.874291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.335131</td>\n",
       "      <td>0.870295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for google-bert/bert-base-uncased: 0.8742905263157895\n"
     ]
    }
   ],
   "source": [
    "# Finetune and evaluate each model on a subset of the data\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "results = {}\n",
    "subset_size = 2000  # Use a small subset for quick comparison\n",
    "\n",
    "model_ckpt = model_names['bert']\n",
    "print(f'\\n===== Finetuning {model_ckpt} =====')\n",
    "# Use use_fast=False to avoid SentencePiece conversion errors\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "# Tokenize subset\n",
    "train_subset = train_dataset.select(range(subset_size))\n",
    "val_subset = val_dataset.select(range(500))\n",
    "tokenized_train = train_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{model_ckpt.replace(\"/\", \"_\")}',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy='steps',   # <-- FIX: Set both strategies to 'steps'\n",
    "    save_strategy='steps',         # <-- FIX: Set both strategies to 'steps'\n",
    "    eval_steps=50,                 # <-- Evaluate every 50 steps\n",
    "    save_steps=50,                 # <-- Save every 50 steps\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_f1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "results[model_ckpt] = eval_result['eval_f1']\n",
    "print(f'F1 score for {model_ckpt}:', eval_result['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07767392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Finetuning FacebookAI/roberta-base =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshit\\.conda\\envs\\snlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kshit\\.cache\\huggingface\\hub\\models--FacebookAI--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2335.69 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2637.79 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/250 01:47 < 01:12, 1.38 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.647600</td>\n",
       "      <td>0.400483</td>\n",
       "      <td>0.853822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.356182</td>\n",
       "      <td>0.899213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.441154</td>\n",
       "      <td>0.900217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for FacebookAI/roberta-base: 0.8992131651698811\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = model_names['roberta']\n",
    "print(f'\\n===== Finetuning {model_ckpt} =====')\n",
    "# Use use_fast=False to avoid SentencePiece conversion errors\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "# Tokenize subset\n",
    "train_subset = train_dataset.select(range(subset_size))\n",
    "val_subset = val_dataset.select(range(500))\n",
    "tokenized_train = train_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{model_ckpt.replace(\"/\", \"_\")}',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy='steps',   # <-- FIX: Set both strategies to 'steps'\n",
    "    save_strategy='steps',         # <-- FIX: Set both strategies to 'steps'\n",
    "    eval_steps=50,                 # <-- Evaluate every 50 steps\n",
    "    save_steps=50,                 # <-- Save every 50 steps\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_f1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "results[model_ckpt] = eval_result['eval_f1']\n",
    "print(f'F1 score for {model_ckpt}:', eval_result['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a3ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Finetuning google/electra-small-discriminator =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshit\\.conda\\envs\\snlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kshit\\.cache\\huggingface\\hub\\models--google--electra-small-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1757.68 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1502.46 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.685363</td>\n",
       "      <td>0.396925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.676780</td>\n",
       "      <td>0.504568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.666409</td>\n",
       "      <td>0.651035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.642896</td>\n",
       "      <td>0.698613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.623895</td>\n",
       "      <td>0.766896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for google/electra-small-discriminator: 0.7668961053525369\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = model_names['electra']\n",
    "print(f'\\n===== Finetuning {model_ckpt} =====')\n",
    "# Use use_fast=False to avoid SentencePiece conversion errors\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "# Tokenize subset\n",
    "train_subset = train_dataset.select(range(subset_size))\n",
    "val_subset = val_dataset.select(range(500))\n",
    "tokenized_train = train_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{model_ckpt.replace(\"/\", \"_\")}',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy='steps',   # <-- FIX: Set both strategies to 'steps'\n",
    "    save_strategy='steps',         # <-- FIX: Set both strategies to 'steps'\n",
    "    eval_steps=50,                 # <-- Evaluate every 50 steps\n",
    "    save_steps=50,                 # <-- Save every 50 steps\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_f1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "results[model_ckpt] = eval_result['eval_f1']\n",
    "print(f'F1 score for {model_ckpt}:', eval_result['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560bbf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Finetuning jhu-clsp/ettin-encoder-17m =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshit\\.conda\\envs\\snlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kshit\\.cache\\huggingface\\hub\\models--jhu-clsp--ettin-encoder-17m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/ettin-encoder-17m and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 4363.38 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3860.87 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/250 00:11 < 00:08, 12.38 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.579469</td>\n",
       "      <td>0.693205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.497668</td>\n",
       "      <td>0.748438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.610944</td>\n",
       "      <td>0.656288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for jhu-clsp/ettin-encoder-17m: 0.7484377251672673\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = model_names['ettin']\n",
    "print(f'\\n===== Finetuning {model_ckpt} =====')\n",
    "# Use use_fast=False to avoid SentencePiece conversion errors\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "# Tokenize subset\n",
    "train_subset = train_dataset.select(range(subset_size))\n",
    "val_subset = val_dataset.select(range(500))\n",
    "tokenized_train = train_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{model_ckpt.replace(\"/\", \"_\")}',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy='steps',   # <-- FIX: Set both strategies to 'steps'\n",
    "    save_strategy='steps',         # <-- FIX: Set both strategies to 'steps'\n",
    "    eval_steps=50,                 # <-- Evaluate every 50 steps\n",
    "    save_steps=50,                 # <-- Save every 50 steps\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_f1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "results[model_ckpt] = eval_result['eval_f1']\n",
    "print(f'F1 score for {model_ckpt}:', eval_result['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e46845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Finetuning thenlper/gte-small =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshit\\.conda\\envs\\snlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kshit\\.cache\\huggingface\\hub\\models--thenlper--gte-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at thenlper/gte-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 3891.90 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3305.83 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/250 00:45 < 00:30, 3.28 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.520883</td>\n",
       "      <td>0.912089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.306399</td>\n",
       "      <td>0.924028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.319200</td>\n",
       "      <td>0.359289</td>\n",
       "      <td>0.867890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for thenlper/gte-small: 0.9240282067418631\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = model_names['gte']\n",
    "print(f'\\n===== Finetuning {model_ckpt} =====')\n",
    "# Use use_fast=False to avoid SentencePiece conversion errors\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "# Tokenize subset\n",
    "train_subset = train_dataset.select(range(subset_size))\n",
    "val_subset = val_dataset.select(range(500))\n",
    "tokenized_train = train_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_subset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{model_ckpt.replace(\"/\", \"_\")}',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy='steps',   # <-- FIX: Set both strategies to 'steps'\n",
    "    save_strategy='steps',         # <-- FIX: Set both strategies to 'steps'\n",
    "    eval_steps=50,                 # <-- Evaluate every 50 steps\n",
    "    save_steps=50,                 # <-- Save every 50 steps\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_f1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "results[model_ckpt] = eval_result['eval_f1']\n",
    "print(f'F1 score for {model_ckpt}:', eval_result['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f8e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All model F1 scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thenlper/gte-small</th>\n",
       "      <td>0.924028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FacebookAI/roberta-base</th>\n",
       "      <td>0.899213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google-bert/bert-base-uncased</th>\n",
       "      <td>0.874291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/electra-small-discriminator</th>\n",
       "      <td>0.766896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jhu-clsp/ettin-encoder-17m</th>\n",
       "      <td>0.748438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    F1 Score\n",
       "thenlper/gte-small                  0.924028\n",
       "FacebookAI/roberta-base             0.899213\n",
       "google-bert/bert-base-uncased       0.874291\n",
       "google/electra-small-discriminator  0.766896\n",
       "jhu-clsp/ettin-encoder-17m          0.748438"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display all F1 scores in a sorted table\n",
    "results = pd.DataFrame.from_dict(results, orient='index', columns=['F1 Score']).sort_values(by='F1 Score', ascending=False)\n",
    "print('\\nAll model F1 scores:')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69214e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on F1 score\n",
    "best_model_ckpt = 'thenlper/gte-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2bfae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finetuning best model thenlper/gte-small on the full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at thenlper/gte-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 40500/40500 [02:45<00:00, 244.00 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:19<00:00, 260.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1251' max='10126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1251/10126 18:02 < 2:08:11, 1.15 it/s, Epoch 0.25/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>0.518654</td>\n",
       "      <td>0.897784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.316876</td>\n",
       "      <td>0.903045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>0.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.244927</td>\n",
       "      <td>0.919710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.249364</td>\n",
       "      <td>0.920988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.295854</td>\n",
       "      <td>0.900108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.280737</td>\n",
       "      <td>0.918473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.242212</td>\n",
       "      <td>0.924997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.219163</td>\n",
       "      <td>0.928999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>0.251821</td>\n",
       "      <td>0.924732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.247052</td>\n",
       "      <td>0.915584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.221505</td>\n",
       "      <td>0.929392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.259756</td>\n",
       "      <td>0.921095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.304296</td>\n",
       "      <td>0.903987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.293800</td>\n",
       "      <td>0.215024</td>\n",
       "      <td>0.927569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>0.219232</td>\n",
       "      <td>0.932999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.263661</td>\n",
       "      <td>0.919043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.209188</td>\n",
       "      <td>0.930785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.235238</td>\n",
       "      <td>0.931398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.239311</td>\n",
       "      <td>0.929162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.225900</td>\n",
       "      <td>0.933598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.275761</td>\n",
       "      <td>0.920611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.224623</td>\n",
       "      <td>0.934186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.240589</td>\n",
       "      <td>0.930718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 46/313 00:05 < 00:30, 8.67 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finetune the best model on the full training set and evaluate on the test set\n",
    "print(f'\\nFinetuning best model {best_model_ckpt} on the full dataset...')\n",
    "\n",
    "# Reload tokenizer and model for best checkpoint\n",
    "best_tokenizer = AutoTokenizer.from_pretrained(best_model_ckpt, use_fast=False)\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_ckpt, num_labels=2)\n",
    "\n",
    "# Tokenize full train and test sets\n",
    "full_tokenized_train = train_dataset.map(lambda x: preprocess_function(x, best_tokenizer), batched=True)\n",
    "full_tokenized_test = test_dataset.map(lambda x: preprocess_function(x, best_tokenizer), batched=True)\n",
    "\n",
    "full_tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "full_tokenized_test.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "full_training_args = TrainingArguments(\n",
    "    output_dir=f'./results/best_full_{best_model_ckpt.replace(\"/\", \"_\")}',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy='steps',   # <-- FIX: Set both strategies to 'steps'\n",
    "    save_strategy='steps',         # <-- FIX: Set both strategies to 'steps'\n",
    "    eval_steps=50,                 # <-- Evaluate every 50 steps\n",
    "    save_steps=50,                 # <-- Save every 50 steps\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "full_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=full_training_args,\n",
    "    train_dataset=full_tokenized_train,\n",
    "    eval_dataset=full_tokenized_test,\n",
    "    compute_metrics=compute_f1\n",
    ")\n",
    "\n",
    "full_trainer.train()\n",
    "full_eval_result = full_trainer.evaluate()\n",
    "print(f'\\nTest F1 score for best model: {full_eval_result[\"eval_f1\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b90653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 1 ---\n",
      "Review: When I was young I had seen very few movies. My parents in all their wisdom rented this one. I was very wary of what the movie was about, in fact I wasn't even allowed to watch it. My brother and sister got to of course and this made me very angry. So what did I do? Late at night I trashed the VCR! Kicked the screen of the TV in and called the police and reported vandals. I was arrested of course, I was unable to get my foot out of the TV set before the police arrived. I was only given a stern t...\n",
      "True label: positive | Predicted label: 1\n",
      "\n",
      "--- Sample 2 ---\n",
      "Review: - When the local sheriff is killed, his wife takes over until and is determined to clean-up the town. Not everyone in town, however, is happy with what she's doing. When the sheriff orders a curfew in town, the local saloon owner (also a woman) hires a killer to take care of the sheriff. There's no way the saloon owner could know that the sheriff and the killer would fall in love.<br /><br />- Gunslinger is an example of what happens when you have a fairly interesting concept and combine it with...\n",
      "True label: negative | Predicted label: 0\n",
      "\n",
      "--- Sample 3 ---\n",
      "Review: I've been watching Buffy the Vampire Slayer and really didn't begin to love the show until Season 4 started. This episode \"Hush\" was view by me alone in the dark just after Midnight with my windows open and the wind blowing through furiously by an after storm. The writers of this episode did an excellent job scaring the heck out of me. I was in awe the entire episode which I just finished 2 minutes ago. Amazing doesn't touch the surface of what this episode accomplishes with almost no dialogue. ...\n",
      "True label: positive | Predicted label: 1\n",
      "\n",
      "--- Sample 4 ---\n",
      "Review: \"The Thomas Crown Affair\" is a terrible remake of a not-very-good movie, redeemable only for the topless shots of former supermodel Renee Russo.<br /><br />That's it. The plot is negligible, Pierce Brosnan phoned in his part, and Dennis Leary (as usual) plays an annoying Irish cop, but I couldn't take my eyes off the beautiful Ms. Russo. There's an okay love-making scene on a stairway, a terrifically sexy ballroom dance, a topless beach scene, and a roll in the sack. Oh, and there's a painting s...\n",
      "True label: negative | Predicted label: 0\n",
      "\n",
      "--- Sample 5 ---\n",
      "Review: This is a film that everyone should watch. Quite apart from raising hugely important points (while South Africa is on the road to recovery there are still many countries in similar situations now), it is superbly directed while Denzel Washington gives, in my opinion, the best performance in his career so far. Kline also gives a good performance, although perhaps not as stunning as Washington's. John Thaw also puts in a good turn as the Chief of Police.<br /><br />There are so many possible areas...\n",
      "True label: positive | Predicted label: 1\n",
      "\n",
      "--- Sample 6 ---\n",
      "Review: This is a movie that will leave you thinking, is he or isn't he? While many people have complained about the ambiguous ending, it gives room for the audience to think and interpret it from the signs. This is my interpretation and theory, and I believe it is very sound. <br /><br />First, here is the plot. One day, Prot (Kevin Spacey) suddenly appears in the midst of a busy train station. After attempting to help a woman from muggers, he is arrested and sent to Bellevue, and later transferred to ...\n",
      "True label: positive | Predicted label: 1\n",
      "\n",
      "--- Sample 7 ---\n",
      "Review: I hated this film. Simply put, this film is so bad that I almost want to disregard ever watching it and never again mentioning it. But on the other hand, I can't resist a good bashing. And if there's one thing that Evan Almighty does for the audience it is that it brings out the best criticism.<br /><br />The film (a sequel to the much funnier Bruce Almighty) starts out by reintroducing the audience to Evan Baxter, a mere supporting at best character in the original film. That's right. This film...\n",
      "True label: negative | Predicted label: 0\n",
      "\n",
      "--- Sample 8 ---\n",
      "Review: I never intended to see Venom, but I caught it on cable. It does have good elements. The Louisiana swamp atmosphere for one, something we will unfortunately not see so much of in movies because of Hurricane Katrina. It is based on an interesting concept, a regular man imbued with the spirits of evil. His confrontation with his son could have been interesting, as could much of the movie. But as tends to happen in Hollywood, an interesting idea goes down a familiar direction: <br /><br />Kill off ...\n",
      "True label: negative | Predicted label: 0\n",
      "\n",
      "--- Sample 9 ---\n",
      "Review: Burt Reynolds plays Gator McKlusky, a likable ex-convict just released from prison who helps the feds nab a corrupt small town sheriff. Laid-back Reynolds was often accused by critics of merely phoning these 'good ol' boy' performances in; true, he's on auto-pilot throughout. But in his day, Reynolds knew just how to make a low-key effort work well for himself. Ingratiating and handsome, Reynolds comes as close to winking at the audience as he can without breaking up; he seems to know these back...\n",
      "True label: negative | Predicted label: 1\n",
      "\n",
      "--- Sample 10 ---\n",
      "Review: (aka: The Bloodsucker Leads the Dance)<br /><br />Lots of naked babes in this one with a couple of lesbo scenes thrown in. The film is supposed to take place in Ireland but it looks more like Rome and the Adriatic to me.<br /><br />Gothic lesbians get invited to a Count's island castle for the weekend. One by one they seem to be missing their heads due to a madperson running around.<br /><br />It's not very scary or bloody and the rooms look like they are lit with floodlights even though candles...\n",
      "True label: negative | Predicted label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on 10 randomly sampled reviews from the test set\n",
    "num_samples = 10\n",
    "sample_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "samples = [test_dataset[i] for i in sample_indices]\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    text = sample['review']\n",
    "    true_label = sample['sentiment']\n",
    "    inputs = best_tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=256)\n",
    "    inputs = {k: v.to(best_model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(**inputs)\n",
    "        pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "    print(f'--- Sample {i+1} ---')\n",
    "    print('Review:', text[:500] + ('...' if len(text) > 500 else ''))\n",
    "    print('True label:', true_label, '| Predicted label:', pred_label)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
