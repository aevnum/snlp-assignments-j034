{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6277,"databundleVersionId":323734,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:10:03.809208Z","iopub.execute_input":"2025-09-06T21:10:03.809458Z","iopub.status.idle":"2025-09-06T21:10:05.886143Z","shell.execute_reply.started":"2025-09-06T21:10:03.809423Z","shell.execute_reply":"2025-09-06T21:10:05.885365Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/quora-question-pairs/train.csv.zip\n/kaggle/input/quora-question-pairs/sample_submission.csv.zip\n/kaggle/input/quora-question-pairs/test.csv\n/kaggle/input/quora-question-pairs/test.csv.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport zipfile\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Assignment 5: Quora Duplicate Question Detection (Optimized)\")\nprint(\"=\" * 60)\n\n# Step 1: Data Loading and Exploration\nprint(\"\\n1. Loading and Exploring Data\")\nprint(\"-\" * 30)\n\n# Extract and load the data\nwith zipfile.ZipFile('/kaggle/input/quora-question-pairs/train.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('/kaggle/working/')\n\nwith zipfile.ZipFile('/kaggle/input/quora-question-pairs/test.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('/kaggle/working/')\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/working/train.csv')\ntest_df = pd.read_csv('/kaggle/working/test.csv')\n\nprint(f\"Train dataset shape: {train_df.shape}\")\nprint(f\"Test dataset shape: {test_df.shape}\")\nprint(f\"\\nTrain columns: {list(train_df.columns)}\")\nprint(f\"Test columns: {list(test_df.columns)}\")\n\n# Data exploration\nprint(f\"\\nClass distribution in training data:\")\nprint(train_df['is_duplicate'].value_counts())\nprint(f\"Duplicate percentage: {train_df['is_duplicate'].mean():.2%}\")\n\n# Check for missing values\nprint(f\"\\nMissing values in train:\")\nprint(train_df.isnull().sum())\n\n# Display sample data\nprint(f\"\\nSample data:\")\nprint(train_df.head())\n\n# Step 2: Data Preprocessing\nprint(\"\\n2. Data Preprocessing\")\nprint(\"-\" * 30)\n\ndef preprocess_text(text):\n    \"\"\"Basic text preprocessing\"\"\"\n    if pd.isna(text):\n        return \"\"\n    text = str(text).lower()\n    # Remove extra whitespaces\n    text = ' '.join(text.split())\n    return text\n\n# Apply preprocessing\ntrain_df['question1_clean'] = train_df['question1'].apply(preprocess_text)\ntrain_df['question2_clean'] = train_df['question2'].apply(preprocess_text)\n\n# Remove rows with empty questions\ntrain_df = train_df[(train_df['question1_clean'] != \"\") & (train_df['question2_clean'] != \"\")]\n\nprint(f\"Dataset shape after cleaning: {train_df.shape}\")\n\n# Create train/validation/test splits - USING SMALLER SUBSET FOR SPEED\n# Use only 10% of data for much faster training\nsample_size = min(20000, len(train_df))  # Reduced from 50k to 20k\ntrain_sample = train_df.sample(n=sample_size, random_state=42)\n\nprint(f\"Using sample size: {sample_size} (for faster training)\")\n\n# Split into train/val/test (60/20/20)\ntrain_data, temp_data = train_test_split(train_sample, test_size=0.4, random_state=42, stratify=train_sample['is_duplicate'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['is_duplicate'])\n\nprint(f\"Train size: {len(train_data)}\")\nprint(f\"Validation size: {len(val_data)}\")\nprint(f\"Test size: {len(test_data)} (constant for all experiments)\")\n\n# For fine-tuning experiments, use only 10% of training data\nTRAINING_SUBSET_RATIO = 0.1\ntraining_subset_size = int(len(train_data) * TRAINING_SUBSET_RATIO)\ntrain_subset = train_data.sample(n=training_subset_size, random_state=42)\n\nprint(f\"\\nFor fine-tuning experiments:\")\nprint(f\"Using {TRAINING_SUBSET_RATIO*100}% of training data: {len(train_subset)} samples\")\n\n# Step 3: Helper Functions\nprint(\"\\n3. Setting up Helper Functions\")\nprint(\"-\" * 30)\n\ndef calculate_metrics(y_true, y_pred):\n    \"\"\"Calculate evaluation metrics\"\"\"\n    f1 = f1_score(y_true, y_pred)\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    \n    return {\n        'f1_score': f1,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall\n    }\n\ndef predict_from_similarity(similarities, threshold=0.5):\n    \"\"\"Convert similarity scores to binary predictions\"\"\"\n    return (similarities >= threshold).astype(int)\n\ndef find_best_threshold(similarities, y_true):\n    \"\"\"Find best threshold based on F1 score\"\"\"\n    best_threshold = 0.5\n    best_f1 = 0\n    \n    for threshold in np.arange(0.1, 0.9, 0.05):\n        y_pred = predict_from_similarity(similarities, threshold)\n        f1 = f1_score(y_true, y_pred)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = threshold\n    \n    return best_threshold, best_f1\n\n# Results storage\nresults = {}\n\n# Step 4: Experiment 1 - Benchmark with Default Weights\nprint(\"\\n4. Experiment 1: Benchmark with Default Weights\")\nprint(\"-\" * 50)\n\n# Load pre-trained model\nmodel_name = 'all-MiniLM-L6-v2'\nbenchmark_model = SentenceTransformer(model_name)\n\nprint(f\"Using model: {model_name}\")\n\n# Encode validation questions\nprint(\"Encoding validation questions...\")\nval_q1_embeddings = benchmark_model.encode(val_data['question1_clean'].tolist(), show_progress_bar=True)\nval_q2_embeddings = benchmark_model.encode(val_data['question2_clean'].tolist(), show_progress_bar=True)\n\n# Calculate cosine similarities\nfrom sklearn.metrics.pairwise import cosine_similarity\nval_similarities = []\nfor i in range(len(val_q1_embeddings)):\n    sim = cosine_similarity([val_q1_embeddings[i]], [val_q2_embeddings[i]])[0][0]\n    val_similarities.append(sim)\n\nval_similarities = np.array(val_similarities)\n\n# Find best threshold\nbest_threshold, best_val_f1 = find_best_threshold(val_similarities, val_data['is_duplicate'].values)\nprint(f\"Best threshold on validation: {best_threshold:.3f} (F1: {best_val_f1:.3f})\")\n\n# Test on test set\nprint(\"Encoding test questions...\")\ntest_q1_embeddings = benchmark_model.encode(test_data['question1_clean'].tolist(), show_progress_bar=True)\ntest_q2_embeddings = benchmark_model.encode(test_data['question2_clean'].tolist(), show_progress_bar=True)\n\ntest_similarities = []\nfor i in range(len(test_q1_embeddings)):\n    sim = cosine_similarity([test_q1_embeddings[i]], [test_q2_embeddings[i]])[0][0]\n    test_similarities.append(sim)\n\ntest_similarities = np.array(test_similarities)\ntest_predictions = predict_from_similarity(test_similarities, best_threshold)\n\n# Calculate metrics\nbenchmark_metrics = calculate_metrics(test_data['is_duplicate'].values, test_predictions)\nresults['Benchmark (Default)'] = benchmark_metrics\n\nprint(f\"Benchmark Results:\")\nfor metric, value in benchmark_metrics.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Step 5: Experiment 2 - Bi-encoder with Cosine Similarity Loss (10% data)\nprint(\"\\n5. Experiment 2: Bi-encoder with Cosine Similarity Loss (10% training data)\")\nprint(\"-\" * 70)\n\n# Prepare training data for sentence transformers using SUBSET\ndef prepare_training_data(df):\n    examples = []\n    for _, row in df.iterrows():\n        score = float(row['is_duplicate'])  # Convert to similarity score\n        example = InputExample(texts=[row['question1_clean'], row['question2_clean']], label=score)\n        examples.append(example)\n    return examples\n\ntrain_examples = prepare_training_data(train_subset)  # Using subset now\nprint(f\"Created {len(train_examples)} training examples (10% of original)\")\n\n# Initialize model for fine-tuning\ncosine_model = SentenceTransformer(model_name)\n\n# Create data loader with larger batch size for faster training\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)  # Increased batch size\n\n# Define loss function\ntrain_loss = losses.CosineSimilarityLoss(cosine_model)\n\n# Fine-tune the model with fewer epochs\nprint(\"Fine-tuning with Cosine Similarity Loss...\")\ncosine_model.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=1,  # Further reduced epochs\n    warmup_steps=50,  # Reduced warmup steps\n    show_progress_bar=True\n)\n\n# Evaluate\nprint(\"Evaluating Cosine Similarity model...\")\nval_q1_embeddings = cosine_model.encode(val_data['question1_clean'].tolist())\nval_q2_embeddings = cosine_model.encode(val_data['question2_clean'].tolist())\n\nval_similarities = []\nfor i in range(len(val_q1_embeddings)):\n    sim = cosine_similarity([val_q1_embeddings[i]], [val_q2_embeddings[i]])[0][0]\n    val_similarities.append(sim)\n\nval_similarities = np.array(val_similarities)\nbest_threshold, _ = find_best_threshold(val_similarities, val_data['is_duplicate'].values)\n\n# Test evaluation\ntest_q1_embeddings = cosine_model.encode(test_data['question1_clean'].tolist())\ntest_q2_embeddings = cosine_model.encode(test_data['question2_clean'].tolist())\n\ntest_similarities = []\nfor i in range(len(test_q1_embeddings)):\n    sim = cosine_similarity([test_q1_embeddings[i]], [test_q2_embeddings[i]])[0][0]\n    test_similarities.append(sim)\n\ntest_similarities = np.array(test_similarities)\ntest_predictions = predict_from_similarity(test_similarities, best_threshold)\n\ncosine_metrics = calculate_metrics(test_data['is_duplicate'].values, test_predictions)\nresults['Bi-encoder (Cosine Loss)'] = cosine_metrics\n\nprint(f\"Cosine Similarity Loss Results:\")\nfor metric, value in cosine_metrics.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Step 6: Experiment 3 - Bi-encoder with Contrastive Loss (10% data)\nprint(\"\\n6. Experiment 3: Bi-encoder with Contrastive Loss (10% training data)\")\nprint(\"-\" * 65)\n\n# Initialize model for contrastive loss\ncontrastive_model = SentenceTransformer(model_name)\n\n# Prepare data for contrastive loss (needs positive/negative pairs) using subset\ncontrastive_examples = []\nfor _, row in train_subset.iterrows():  # Using subset\n    label = int(row['is_duplicate'])\n    example = InputExample(texts=[row['question1_clean'], row['question2_clean']], label=label)\n    contrastive_examples.append(example)\n\ncontrastive_dataloader = DataLoader(contrastive_examples, shuffle=True, batch_size=32)\n\n# Define contrastive loss\ncontrastive_loss = losses.ContrastiveLoss(contrastive_model)\n\n# Fine-tune\nprint(\"Fine-tuning with Contrastive Loss...\")\ncontrastive_model.fit(\n    train_objectives=[(contrastive_dataloader, contrastive_loss)],\n    epochs=1,\n    warmup_steps=50,\n    show_progress_bar=True\n)\n\n# Evaluate contrastive model\nprint(\"Evaluating Contrastive Loss model...\")\nval_q1_embeddings = contrastive_model.encode(val_data['question1_clean'].tolist())\nval_q2_embeddings = contrastive_model.encode(val_data['question2_clean'].tolist())\n\nval_similarities = []\nfor i in range(len(val_q1_embeddings)):\n    sim = cosine_similarity([val_q1_embeddings[i]], [val_q2_embeddings[i]])[0][0]\n    val_similarities.append(sim)\n\nval_similarities = np.array(val_similarities)\nbest_threshold, _ = find_best_threshold(val_similarities, val_data['is_duplicate'].values)\n\n# Test evaluation\ntest_q1_embeddings = contrastive_model.encode(test_data['question1_clean'].tolist())\ntest_q2_embeddings = contrastive_model.encode(test_data['question2_clean'].tolist())\n\ntest_similarities = []\nfor i in range(len(test_q1_embeddings)):\n    sim = cosine_similarity([test_q1_embeddings[i]], [test_q2_embeddings[i]])[0][0]\n    test_similarities.append(sim)\n\ntest_similarities = np.array(test_similarities)\ntest_predictions = predict_from_similarity(test_similarities, best_threshold)\n\ncontrastive_metrics = calculate_metrics(test_data['is_duplicate'].values, test_predictions)\nresults['Bi-encoder (Contrastive Loss)'] = contrastive_metrics\n\nprint(f\"Contrastive Loss Results:\")\nfor metric, value in contrastive_metrics.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Step 7: Experiment 4 - Multiple Negative Ranking Loss (10% data)\nprint(\"\\n7. Experiment 4: Bi-encoder with Multiple Negative Ranking Loss (10% training data)\")\nprint(\"-\" * 75)\n\n# Initialize model for MNR loss\nmnr_model = SentenceTransformer(model_name)\n\n# Prepare data for MNR loss (only positive pairs) from subset\nmnr_examples = []\npositive_pairs = train_subset[train_subset['is_duplicate'] == 1]  # Using subset\nfor _, row in positive_pairs.iterrows():\n    example = InputExample(texts=[row['question1_clean'], row['question2_clean']])\n    mnr_examples.append(example)\n\nprint(f\"Using {len(mnr_examples)} positive pairs for MNR loss (from 10% subset)\")\n\nmnr_dataloader = DataLoader(mnr_examples, shuffle=True, batch_size=32)\n\n# Define MNR loss\nmnr_loss = losses.MultipleNegativesRankingLoss(mnr_model)\n\n# Fine-tune\nprint(\"Fine-tuning with Multiple Negative Ranking Loss...\")\nmnr_model.fit(\n    train_objectives=[(mnr_dataloader, mnr_loss)],\n    epochs=1,\n    warmup_steps=50,\n    show_progress_bar=True\n)\n\n# Evaluate MNR model\nprint(\"Evaluating MNR Loss model...\")\nval_q1_embeddings = mnr_model.encode(val_data['question1_clean'].tolist())\nval_q2_embeddings = mnr_model.encode(val_data['question2_clean'].tolist())\n\nval_similarities = []\nfor i in range(len(val_q1_embeddings)):\n    sim = cosine_similarity([val_q1_embeddings[i]], [val_q2_embeddings[i]])[0][0]\n    val_similarities.append(sim)\n\nval_similarities = np.array(val_similarities)\nbest_threshold, _ = find_best_threshold(val_similarities, val_data['is_duplicate'].values)\n\n# Test evaluation\ntest_q1_embeddings = mnr_model.encode(test_data['question1_clean'].tolist())\ntest_q2_embeddings = mnr_model.encode(test_data['question2_clean'].tolist())\n\ntest_similarities = []\nfor i in range(len(test_q1_embeddings)):\n    sim = cosine_similarity([test_q1_embeddings[i]], [test_q2_embeddings[i]])[0][0]\n    test_similarities.append(sim)\n\ntest_similarities = np.array(test_similarities)\ntest_predictions = predict_from_similarity(test_similarities, best_threshold)\n\nmnr_metrics = calculate_metrics(test_data['is_duplicate'].values, test_predictions)\nresults['Bi-encoder (MNR Loss)'] = mnr_metrics\n\nprint(f\"Multiple Negative Ranking Loss Results:\")\nfor metric, value in mnr_metrics.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Step 8: Experiment 5 - Cross-encoder (10% data)\nprint(\"\\n8. Experiment 5: Cross-encoder (10% training data)\")\nprint(\"-\" * 50)\n\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\n# Initialize cross-encoder\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', num_labels=2)\n\n# Prepare training data for cross-encoder using subset\nce_train_samples = []\nfor _, row in train_subset.iterrows():  # Using subset\n    ce_train_samples.append([row['question1_clean'], row['question2_clean'], int(row['is_duplicate'])])\n\nprint(f\"Created {len(ce_train_samples)} cross-encoder training samples (from 10% subset)\")\n\n# Fine-tune cross-encoder\nprint(\"Fine-tuning Cross-encoder...\")\ncross_encoder.fit(\n    train_samples=ce_train_samples,\n    epochs=1,\n    batch_size=32,\n    warmup_steps=50,\n    show_progress_bar=True\n)\n\n# Evaluate cross-encoder\nprint(\"Evaluating Cross-encoder...\")\ntest_pairs = [[row['question1_clean'], row['question2_clean']] for _, row in test_data.iterrows()]\nce_predictions = cross_encoder.predict(test_pairs)\n\n# Convert probabilities to binary predictions\nce_binary_predictions = (ce_predictions > 0.5).astype(int)\n\nce_metrics = calculate_metrics(test_data['is_duplicate'].values, ce_binary_predictions)\nresults['Cross-encoder'] = ce_metrics\n\nprint(f\"Cross-encoder Results:\")\nfor metric, value in ce_metrics.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Step 9: Results Comparison\nprint(\"\\n9. Final Results Comparison\")\nprint(\"=\" * 50)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(results).T\nprint(results_df.round(4))\n\n# Plot F1 scores\nplt.figure(figsize=(12, 6))\nmodels = list(results.keys())\nf1_scores = [results[model]['f1_score'] for model in models]\n\nplt.subplot(1, 2, 1)\nbars = plt.bar(range(len(models)), f1_scores)\nplt.xlabel('Models')\nplt.ylabel('F1 Score')\nplt.title('F1 Score Comparison')\nplt.xticks(range(len(models)), models, rotation=45, ha='right')\nplt.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor i, bar in enumerate(bars):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n             f'{f1_scores[i]:.3f}', ha='center', va='bottom')\n\n# Plot all metrics\nplt.subplot(1, 2, 2)\nmetrics_to_plot = ['f1_score', 'accuracy', 'precision', 'recall']\nx_pos = np.arange(len(models))\nwidth = 0.2\n\nfor i, metric in enumerate(metrics_to_plot):\n    values = [results[model][metric] for model in models]\n    plt.bar(x_pos + i*width, values, width, label=metric.replace('_', ' ').title())\n\nplt.xlabel('Models')\nplt.ylabel('Score')\nplt.title('All Metrics Comparison')\nplt.xticks(x_pos + width*1.5, models, rotation=45, ha='right')\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Best performing model\nbest_model = max(results, key=lambda x: results[x]['f1_score'])\nprint(f\"\\nBest performing model: {best_model}\")\nprint(f\"Best F1 Score: {results[best_model]['f1_score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T03:34:34.517510Z","iopub.execute_input":"2025-09-07T03:34:34.518163Z","execution_failed":"2025-09-07T03:42:39.921Z"}},"outputs":[{"name":"stderr","text":"2025-09-07 03:35:01.352897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757216101.709164      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757216101.817880      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Assignment 5: Quora Duplicate Question Detection (Optimized)\n============================================================\n\n1. Loading and Exploring Data\n------------------------------\nTrain dataset shape: (404290, 6)\nTest dataset shape: (3563475, 3)\n\nTrain columns: ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\nTest columns: ['test_id', 'question1', 'question2']\n\nClass distribution in training data:\nis_duplicate\n0    255027\n1    149263\nName: count, dtype: int64\nDuplicate percentage: 36.92%\n\nMissing values in train:\nid              0\nqid1            0\nqid2            0\nquestion1       1\nquestion2       2\nis_duplicate    0\ndtype: int64\n\nSample data:\n   id  qid1  qid2                                          question1  \\\n0   0     1     2  What is the step by step guide to invest in sh...   \n1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2   2     5     6  How can I increase the speed of my internet co...   \n3   3     7     8  Why am I mentally very lonely? How can I solve...   \n4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n\n                                           question2  is_duplicate  \n0  What is the step by step guide to invest in sh...             0  \n1  What would happen if the Indian government sto...             0  \n2  How can Internet speed be increased by hacking...             0  \n3  Find the remainder when [math]23^{24}[/math] i...             0  \n4            Which fish would survive in salt water?             0  \n\n2. Data Preprocessing\n------------------------------\nDataset shape after cleaning: (404287, 8)\nUsing sample size: 20000 (for faster training)\nTrain size: 12000\nValidation size: 4000\nTest size: 4000 (constant for all experiments)\n\nFor fine-tuning experiments:\nUsing 10.0% of training data: 1200 samples\n\n3. Setting up Helper Functions\n------------------------------\n\n4. Experiment 1: Benchmark with Default Weights\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e31737579834704add38748402245be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7551f6acd1844a8b82ec9016bb08300d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791ba2a68deb40e9b76fbe68426a70b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f54963d6d446e3a43b0f6296d91d34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"352ac58370a447ccb4782fa00836c40b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aa9975b142f4cd59933088310b2d898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a18e8fcea96943009844a7b9186448a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04144782ca844701a92b5d5448d7cb78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e1f34b2e25e42a584968fd48fff84d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3235b674a4be407ca5a42ec13820d070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07bf6b6ba212443aba486f668aafecfb"}},"metadata":{}},{"name":"stdout","text":"Using model: all-MiniLM-L6-v2\nEncoding validation questions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c281343ff94f139de43713147c09cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d01d0338cbb442cbc22c9ac051195a6"}},"metadata":{}},{"name":"stdout","text":"Best threshold on validation: 0.750 (F1: 0.741)\nEncoding test questions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"019396fd001e448ea45ecdc617a477d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"259d217d0ef54424a09aede777d37403"}},"metadata":{}},{"name":"stdout","text":"Benchmark Results:\n  f1_score: 0.7285\n  accuracy: 0.7630\n  precision: 0.6269\n  recall: 0.8694\n\n5. Experiment 2: Bi-encoder with Cosine Similarity Loss (10% training data)\n----------------------------------------------------------------------\nCreated 1200 training examples (10% of original)\nFine-tuning with Cosine Similarity Loss...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null}]}