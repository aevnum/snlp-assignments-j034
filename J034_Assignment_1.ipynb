{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfe03c8",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "- Find similar words in the given text using the Word2Vec model.\n",
    "- Come up with examples like \"king - man + woman\" = \"queen\" for word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================------------------] 64.4% 1070.1/1662.8MB downloadedModel loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install gensim and download the pretrained Google News word2vec model\n",
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda817f8",
   "metadata": {},
   "source": [
    "## Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c1cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'Naan':\n",
      "  Paneer (0.697)\n",
      "  Kadhai (0.677)\n",
      "  Idli (0.665)\n",
      "  Paratha (0.662)\n",
      "  Chaat (0.660)\n",
      "Similar words to 'Kshitij':\n",
      "  Paneer (0.697)\n",
      "  Kadhai (0.677)\n",
      "  Idli (0.665)\n",
      "  Paratha (0.662)\n",
      "  Chaat (0.660)\n",
      "Similar words to 'Kshitij':\n",
      "  Gaurav (0.686)\n",
      "  Nikhil (0.679)\n",
      "  Pranay (0.669)\n",
      "  Harshad (0.669)\n",
      "  Rohit (0.668)\n",
      "Similar words to 'horizon':\n",
      "  Gaurav (0.686)\n",
      "  Nikhil (0.679)\n",
      "  Pranay (0.669)\n",
      "  Harshad (0.669)\n",
      "  Rohit (0.668)\n",
      "Similar words to 'horizon':\n",
      "  looming (0.514)\n",
      "  Fireballs_lit (0.514)\n",
      "  distant_speck (0.494)\n",
      "  looms_ominously (0.470)\n",
      "  clouds (0.468)\n",
      "Similar words to 'anime':\n",
      "  looming (0.514)\n",
      "  Fireballs_lit (0.514)\n",
      "  distant_speck (0.494)\n",
      "  looms_ominously (0.470)\n",
      "  clouds (0.468)\n",
      "Similar words to 'anime':\n",
      "  manga (0.809)\n",
      "  animé (0.758)\n",
      "  Anime (0.754)\n",
      "  anime_manga (0.715)\n",
      "  animes (0.702)\n",
      "Similar words to 'Minecraft':\n",
      "  manga (0.809)\n",
      "  animé (0.758)\n",
      "  Anime (0.754)\n",
      "  anime_manga (0.715)\n",
      "  animes (0.702)\n",
      "Similar words to 'Minecraft':\n",
      "  Mojang (0.713)\n",
      "  Markus_Persson (0.684)\n",
      "  LittleBigPlanet (0.652)\n",
      "  Alien_Hominid (0.648)\n",
      "  XBLA (0.633)\n",
      "  Mojang (0.713)\n",
      "  Markus_Persson (0.684)\n",
      "  LittleBigPlanet (0.652)\n",
      "  Alien_Hominid (0.648)\n",
      "  XBLA (0.633)\n"
     ]
    }
   ],
   "source": [
    "# Pick 5 words and find similar words for each\n",
    "words = ['Naan', 'Kshitij', 'horizon', 'anime', 'Minecraft']\n",
    "for word in words:\n",
    "    print(f\"Similar words to '{word}':\")\n",
    "    try:\n",
    "        similar = model.most_similar(word, topn=5)\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word} ({score:.3f})\")\n",
    "    except KeyError:\n",
    "        print(f\"  '{word}' not found in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0544b1",
   "metadata": {},
   "source": [
    "## Word Vector Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d397d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for ['Japan', 'Pasta'] - ['Italy']:\n",
      "  Sushi (0.584)\n",
      "  Teriyaki (0.575)\n",
      "  Tofu (0.529)\n",
      "\n",
      "Result for ['Sword', 'Paper'] - ['Pen']:\n",
      "  Knight_Chronicles (0.424)\n",
      "  Steel (0.401)\n",
      "  Mage (0.383)\n",
      "\n",
      "Result for ['cat', 'beak'] - ['bird']:\n",
      "  paws (0.567)\n",
      "  claws (0.541)\n",
      "  hind_paws (0.534)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    (['Japan', 'Pasta'], ['Italy']),\n",
    "    (['Sword', 'Paper'], ['Pen']),\n",
    "    (['cat', 'beak'], ['bird'])\n",
    " ]\n",
    "for positive, negative in experiments:\n",
    "    try:\n",
    "        result = model.most_similar(positive=positive, negative=negative, topn=3)\n",
    "        print(f\"Result for {positive} - {negative}:\")\n",
    "        for word, score in result:\n",
    "            print(f\"  {word} ({score:.3f})\")\n",
    "    except KeyError as e:\n",
    "        print(f\"  Word not found in vocabulary: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3b38f",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "- Build a movie review sentiment classifier using WordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f591664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a485933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.12).\n",
      "Path to dataset files: C:\\Users\\kshit\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e2abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (50000, 2)\n",
      "Columns: ['review', 'sentiment']\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "Label distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB dataset (CSV file assumed in downloaded path)\n",
    "data_path = os.path.join(path, 'IMDB Dataset.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', df.columns.tolist())\n",
    "print(df.head())\n",
    "print('Label distribution:')\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f659f6",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee24cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kshit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                        clean_review  \n",
      "0  one reviewers mentioned watching oz episode yo...  \n",
      "1  wonderful little production br br filming tech...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically theres family little boy jake thinks...  \n",
      "4  petter matteis love time money visually stunni...  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "print(df[['review', 'clean_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be6024",
   "metadata": {},
   "source": [
    "## Embedding retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25dd80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:08<00:00, 6036.38it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (50000, 300)\n",
      "Labels shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "def get_review_vector(review, model):\n",
    "    words = review.split()\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X = np.array([get_review_vector(text, model) for text in tqdm(df['clean_review'])])\n",
    "y = (df['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "print('Feature shape:', X.shape)\n",
    "print('Labels shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad6c16",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "860da217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      4961\n",
      "           1       0.85      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ccd668",
   "metadata": {},
   "source": [
    "## Skipgram, CBOW, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae516e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6647.73it/s]\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n",
      "100%|██████████| 50000/50000 [00:07<00:00, 6911.04it/s]\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n",
      "100%|██████████| 50000/50000 [00:10<00:00, 4896.34it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenized reviews\n",
    "tokenized_reviews = [text.split() for text in df['clean_review']]\n",
    "\n",
    "# Train Skip-gram model\n",
    "w2v_skipgram = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, sg=1, min_count=2, workers=4)\n",
    "\n",
    "# Train CBOW model\n",
    "w2v_cbow = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, sg=0, min_count=2, workers=4)\n",
    "\n",
    "# Train FastText model\n",
    "ft_model = FastText(sentences=tokenized_reviews, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "def get_custom_vector(review, model):\n",
    "    words = review.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X_skipgram = np.array([get_custom_vector(text, w2v_skipgram) for text in tqdm(df['clean_review'])])\n",
    "X_cbow = np.array([get_custom_vector(text, w2v_cbow) for text in tqdm(df['clean_review'])])\n",
    "X_fasttext = np.array([get_custom_vector(text, ft_model) for text in tqdm(df['clean_review'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a2da2",
   "metadata": {},
   "source": [
    "## Logistic Regression Evaluation on Custom Word2Vec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6931d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skip-gram Accuracy: 0.8781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      4961\n",
      "           1       0.88      0.88      0.88      5039\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "CBOW Accuracy: 0.8608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      4961\n",
      "           1       0.86      0.87      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "CBOW Accuracy: 0.8608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      4961\n",
      "           1       0.86      0.87      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "FastText Accuracy: 0.8481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      4961\n",
      "           1       0.85      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "FastText Accuracy: 0.8481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      4961\n",
      "           1       0.85      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X, y, name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return acc\n",
    "\n",
    "acc_skipgram = train_and_evaluate(X_skipgram, y, 'Skip-gram')\n",
    "acc_cbow = train_and_evaluate(X_cbow, y, 'CBOW')\n",
    "acc_fasttext = train_and_evaluate(X_fasttext, y, 'FastText')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b11d0",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "| Model         | Accuracy | F1 Score |\n",
    "|---------------|----------|----------|\n",
    "| Pre-trained W2V | 0.8515   | 0.85     |\n",
    "| Skip-gram    | 0.8781   | 0.88     |\n",
    "| CBOW         | 0.8608   | 0.86     |\n",
    "| FastText     | 0.8481   | 0.85     |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
